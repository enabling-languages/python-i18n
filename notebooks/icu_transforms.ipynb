{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text transformations &ndash; <span style=\"color: red !important\">Draft</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import icu\n",
    "import el_internationalisation as eli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In its most general sense, text transformations include:\n",
    "\n",
    "* Case mappings and case-folding,\n",
    "* Unicode Normalisation,\n",
    "* Transforms, and the\n",
    "* Bidirectional algorithm (rendering of a text flow)\n",
    "\n",
    "Python, including Pandas, approaches to text transformations include:\n",
    "\n",
    "|Transformation type  |Python |Pandas  |ICU Class |\n",
    "|-------------------- |------ |------- |--------- |\n",
    "|Case operations        |[str.lower()](https://docs.python.org/3/library/stdtypes.html#str.lower), [str.upper()](https://docs.python.org/3/library/stdtypes.html#str.upper), [str.title()](https://docs.python.org/3/library/stdtypes.html#str.title) |[pandas.Series.str.lower()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html), [pandas.Series.str.upper()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.upper.html), [pandas.Series.str.title()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.title.html) |[icu.UnicodeString()](https://unicode-org.github.io/icu-docs/apidoc/dev/icu4c/classicu_1_1UnicodeString.html)  |\n",
    "|Casefolding |[str.casefold()](https://docs.python.org/3/library/stdtypes.html#str.casefold)  |[pandas.Series.str.casefold()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.casefold.html)  |[icu.UnicodeString()](https://unicode-org.github.io/icu-docs/apidoc/dev/icu4c/classicu_1_1UnicodeString.html)  |\n",
    "|Normalization |[unicodedata.normalize()](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize)  |[pandas.Series.str.normalize()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.normalize.html)  |[icu.Normalizer2()](https://unicode-org.github.io/icu-docs/apidoc/dev/icu4c/classicu_1_1Normalizer2.html)  |\n",
    "|Transforms | - | - |[icuTransliterator()](https://unicode-org.github.io/icu-docs/apidoc/dev/icu4c/classicu_1_1Transliterator.html)\n",
    "|Bidirectional algorithm | - | - |ICU C API for UBA  |\n",
    "\n",
    "N.B. I haven't included [str.capitalize()](https://docs.python.org/3/library/stdtypes.html#str.capitalize) or [pandas.Series.str.capitalize()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.capitalize.html) since sentence casing is a typesetting operation rather than a Unicode casing operation. Technically [str.title()](https://docs.python.org/3/library/stdtypes.html#str.title) and [pandas.Series.str.title()](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.title.html) do not conform to the Unicode titlecasing operation, and shouldn't be considering a casing operation in the same sense as the PyICU equivalent.\n",
    "\n",
    "The above table provides a summary of available text transformations, but this nptebook will concentrate on the `icu.Transliterator()` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casing\n",
    "\n",
    "The Unicode Standard makes a distinction between _default casing algorithms_ and tailorings which may include contextual and language specific tailorings, including:\n",
    "\n",
    "* Turkish and Azeri casing rules for _dotted capital I_ and _dotless small i_.\n",
    "* Casing rules for retention of a dot when combining marks are applied to the letetr _i_.\n",
    "* Titlecasing of _IJ_ in Dutch.\n",
    "* Greek uppercasing and removal of certain combining diactritics.\n",
    "* Special titlecasing for orthographies that include word initial caseless letters.\n",
    "* Uppercasing of ß to ẞ.\n",
    "\n",
    "Casing operations can change the length of a string, they are not necessarily reversible, and can be context and language dependent. Additionally, not all lowercase characters have an uppercase equivalent, so an uppercase string can potentially include both lowercase and caseless letters.\n",
    "\n",
    "Python provides simple Unicode casing, that is, Python's casing operations are language and locale insensitive. \n",
    "\n",
    "### Lowercasing\n",
    "\n",
    "Lowercasing is fairly straightforward string operation in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "κένωσις\n"
     ]
    }
   ],
   "source": [
    "el_lexeme = 'ΚΈΝΩΣΙΣ'\n",
    "print(el_lexeme.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But casing behaviour can differ between simple and full casing support.\n",
    "\n",
    "The Turkish uppercase letter <span class=\"codepoint\" translate=\"no\"><bdi lang=\"tr\">&#x0130;</bdi> [<span class=\"uname\" style=\"text-transform: uppercase;\">U+0130 Latin Capital Letter I With Dot Above</span>]</span> lowercases to <span class=\"codepoint\" translate=\"no\"><bdi lang=\"tr\">&#x0069;</bdi> [<span class=\"uname\" style=\"text-transform: uppercase;\">U+0069 Latin Small Letter I</span>]</span> when language sensitive (full) casing is used.\n",
    "\n",
    "But for language insensitive (simple) casing <span class=\"codepoint\" translate=\"no\"><bdi lang=\"tr\">&#x0130;</bdi> [<span class=\"uname\" style=\"text-transform: uppercase;\">U+0130 Latin Capital Letter I With Dot Above</span>]</span> is mapped to <span class=\"codepoint\" translate=\"no\"><bdi lang=\"tr\">&#x0069;</bdi> [<span class=\"uname\" style=\"text-transform: uppercase;\">>U+0069 Latin Small Letter I</span>]</span>, <span class=\"codepoint\" translate=\"no\"><bdi lang=\"tr\">&#x25CC;&#x0307;</bdi> [<span class=\"uname\" style=\"text-transform: uppercase;\">U+0307 Combining Dot Above</span>]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İstanbul: 0130 0073 0074 0061 006E 0062 0075 006C\n",
      "i̇stanbul: 0069 0307 0073 0074 0061 006E 0062 0075 006C\n"
     ]
    }
   ],
   "source": [
    "tr_city = \"İstanbul\"\n",
    "print(f'{tr_city}: {eli.codepoints(tr_city)}')\n",
    "tr_city_lower = tr_city.lower()\n",
    "print(f'{tr_city_lower}: {eli.codepoints(tr_city_lower)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to use [PyICU](https://gitlab.pyicu.org/main/pyicu) for language sensitive casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "istanbul: 0069 0073 0074 0061 006E 0062 0075 006C\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a locale object\n",
    "loc = icu.Locale(\"tr_TR\")\n",
    "# 2. Connvert string to an ICU UnicodeString object\n",
    "us = icu.UnicodeString(tr_city)\n",
    "# 3. Lowercase string\n",
    "tr_city_icu_lower = us.toLower(loc)\n",
    "print(f'{tr_city_icu_lower}: {eli.codepoints(tr_city_icu_lower)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be collapsed into one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "istanbul: 0069 0073 0074 0061 006E 0062 0075 006C\n"
     ]
    }
   ],
   "source": [
    "tr_city_icu_lower2 = icu.UnicodeString(tr_city).toLower(icu.Locale(\"tr_TR\"))\n",
    "print(f'{tr_city_icu_lower2}: {eli.codepoints(tr_city_icu_lower2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, it is possible to use ICU's root locale to get language insensitive casing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i̇stanbul: 0069 0307 0073 0074 0061 006E 0062 0075 006C\n"
     ]
    }
   ],
   "source": [
    "lang_insensitive = icu.UnicodeString(tr_city).toLower(icu.Locale.getRoot())\n",
    "print(f'{lang_insensitive}: {eli.codepoints(lang_insensitive)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uppercase\n",
    "\n",
    "As with lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUSSE (U+0062 U+0075 U+00DF U+0065)\n",
      "DIYARBAKIR (U+0044 U+0069 U+0079 U+0061 U+0072 U+0062 U+0061 U+006B U+0131 U+0072)\n"
     ]
    }
   ],
   "source": [
    "de_lexeme = \"buße\"\n",
    "print(f'{de_lexeme.upper()} ({eli.cp(de_lexeme, prefix=True)})')\n",
    "tr_province = \"Diyarbakır\"\n",
    "print(f'{tr_province.upper()} ({eli.cp(tr_province, prefix=True)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUSSE: 0042 0055 0053 0053 0045\n"
     ]
    }
   ],
   "source": [
    "de_lexeme_icu = icu.UnicodeString(de_lexeme).toUpper(icu.Locale(\"de_DE\"))\n",
    "print(f'{de_lexeme_icu}: {eli.codepoints(de_lexeme_icu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casefolding\n",
    "\n",
    "Casefolding, on the other hand, does not transform text into a specific case, rather it removes case distinctions from strings that are being compared.\n",
    "\n",
    "* Casefolding is language and locale insensitive\n",
    "* It does not preserve normalization forms\n",
    "* Length of string may change\n",
    "* Context dependent casing does not occur\n",
    "* Lowercase mapping is used for most characters, but uppercase mapping is used for some.\n",
    "\n",
    ">Case folding is related to case conversion. However, the main purpose of case folding is to contribute to caseless matching of strings, whereas the main purpose of case conversion is to put strings into a particular cased form.\n",
    "><br>_Unicode Standard Version 15.0_, [Default Case Folding](https://www.unicode.org/versions/Unicode15.0.0/ch03.pdf#G53253)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unicode normalisation\n",
    "\n",
    "Each Unicode character can have one ro more canonically equivalent forms. If we look at the letters a, á, and ậ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ['U+0061']\n",
      "3: ['U+00E1', 'U+0061 U+0341', 'U+0061 U+0301']\n",
      "5: ['U+1EAD', 'U+00E2 U+0323', 'U+0061 U+0302 U+0323', 'U+1EA1 U+0302', 'U+0061 U+0323 U+0302']\n"
     ]
    }
   ],
   "source": [
    "a = eli.canonical_equivalents_str(\"a\")\n",
    "a_acute = eli.canonical_equivalents_str(\"á\")\n",
    "a_circumflex_dotbelow = eli.canonical_equivalents_str(\"ậ\")\n",
    "\n",
    "print(f\"{len(a)}: {a}\")\n",
    "print(f\"{len(a_acute)}: {a_acute}\")\n",
    "print(f\"{len(a_circumflex_dotbelow)}: {a_circumflex_dotbelow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The letter <span class=\"codepoint\" translate=\"no\"><bdi lang=\"und\">&#x0061;</bdi> \n",
    "(<span class=\"uname\" style=\"text-transform: uppercase;\">U+0061 Latin Small Letter A</span>)</span> only has one canonically equivalent form.\n",
    "\n",
    "While the letter <span class=\"codepoint\" translate=\"no\"><bdi lang=\"und\">&#x00E1;</bdi> \n",
    "(<span class=\"uname\" style=\"text-transform: uppercase;\">U+00E1 Latin Small Letter A With Acute</span>)</span> has three canonically equivalent representations, one of which `U+0061 U+0341` uses a deprecated combining diacritic, leaving two canonically equivalent forms: `U+00E1` and `U+0061 U+0301`.\n",
    "\n",
    "When multiple diacritics are involved, canonical equivalence becomes more complex. The letter <span class=\"codepoint\" translate=\"no\"><bdi lang=\"und\">&#x1EAD;</bdi> \n",
    "(<span class=\"uname\" style=\"text-transform: uppercase;\">U+1EAD Latin Small Letter A With Circumflex And Dot Below</span>)</span> five canonically equivalent versions.\n",
    "\n",
    "The Unicode mechanism for handling canonical equivalence is normalisation. With standard string processing it is possible to normalise the string to a prefered form. There are four normalisation forms defined by Unicode, but only two of these should be used with most text. \n",
    "\n",
    "Earlier we discussed the letter <span class=\"codepoint\" translate=\"no\"><bdi lang=\"und\">&#x00E1;</bdi> \n",
    "(<span class=\"uname\" style=\"text-transform: uppercase;\">U+00E1 Latin Small Letter A With Acute</span>)</span>  which has a one codepoint representation `U+00E1` and a two codepoint representation `U+0061 U+0301`. In the first representation a single character consisting of a vowel and diacritic components is represented as a single codepoint. THis is referred to as a precomposed character. \n",
    "\n",
    "The second sequence consists of the vowel followed by a combining diacritic, ie the diacritic is a character in and of itself. This is refered to as a decomposed sequence.\n",
    "\n",
    "Unicode _Normalisation Form D (NFD)_ will decompose character sequences, then canonically order characters, while Unicode _Normalisation Form C (NFC)_ will decompose the character sequence, canonically order characters, then convert the string to its precomposed representation.\n",
    "\n",
    "The _unicodedata_ module provides a function to normalise Unicode strings:\n",
    "\n",
    "```py\n",
    "unicodedata.normalize(form, str)\n",
    "```\n",
    "\n",
    "It is importnt to note, that the version of Unicode that `unicodedata` supports depends on the version of Python you are using. If you need your Unicode support to be current, then you need to always sue the latest version of Unicode or use a drop-in replacement for `unicodedata` that is kept current.\n",
    "\n",
    "Drop-in replacements for `unicodedata` that are updated and support the latest Unicode versions:\n",
    "\n",
    "1. [unicodedata2](https://pypi.org/project/unicodedata2/)\n",
    "2. [unicodedataplus](https://pypi.org/project/unicodedataplus/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original string: ậ (U+00E2 U+0323)\n",
      "NFC string: ậ (U+1EAD)\n",
      "NFD string: ậ (U+0061 U+0323 U+0302)\n"
     ]
    }
   ],
   "source": [
    "import unicodedata as ud\n",
    "vi_grapheme = \"\\u00E2\\u0323\"\n",
    "vi_grapheme_nfc = ud.normalize(\"NFC\", vi_grapheme)\n",
    "vi_grapheme_nfd = ud.normalize(\"NFD\", vi_grapheme)\n",
    "print(f'Original string: {vi_grapheme} ({eli.cp(vi_grapheme, prefix=True)})')\n",
    "print(f'NFC string: {vi_grapheme_nfc} ({eli.cp(vi_grapheme_nfc, prefix=True)})')\n",
    "print(f'NFD string: {vi_grapheme_nfd} ({eli.cp(vi_grapheme_nfd, prefix=True)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyICU](https://gitlab.pyicu.org/main/pyicu) provides a generic function for normalisation, it also provides specific functions for each normlisation form.\n",
    "\n",
    "You first create a `Normalizer2` instance, then use the `normalize()` function on the Normalizer2 instance on the string you wish to normalise.\n",
    "\n",
    "__Generic function:__\n",
    "\n",
    "```py\n",
    "import icu\n",
    "normalizer = icu.Normalizer2.getInstance(None, form, mode)\n",
    "normalizer.normalize(str)\n",
    "```\n",
    "\n",
    "__*form:*__ normalisation form has a value of `nfc`, `nfkc`, or `nfkc_cf`.\n",
    "__*mode*:__ composition mode, has values of `icu.UNormalizationMode2.COMPOSE` or `icu.UNormalizationMode2.DECOMPOSE`.\n",
    "\n",
    "|Normalisation Form |Form specified |Composition mode |\n",
    "|------------------ |-------------- |---------------- |\n",
    "|NFC     |nfc  |icu.UNormalizationMode2.COMPOSE |\n",
    "|NFKC    |nfkc |icu.UNormalizationMode2.COMPOSE |\n",
    "|NFD     |nfc  |icu.UNormalizationMode2.DECOMPOSE |\n",
    "|NKKD    |nfkc |icu.UNormalizationMode2.DECOMPOSE |\n",
    "|NFKC_CF |nfkc_cf |icu.UNormalizationMode2.COMPOSE |\n",
    "\n",
    "For NFC normalisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFC string: ậ (U+1EAD)\n"
     ]
    }
   ],
   "source": [
    "normalizer1 = icu.Normalizer2.getInstance(None, \"nfc\", icu.UNormalizationMode2.COMPOSE)\n",
    "vi_icu_nfc = normalizer1.normalize(vi_grapheme)\n",
    "print(f'NFC string: {vi_icu_nfc} ({eli.cp(vi_icu_nfc, prefix=True)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NFD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFD string: ậ (U+0061 U+0323 U+0302)\n"
     ]
    }
   ],
   "source": [
    "normalizer2 = icu.Normalizer2.getInstance(None, \"nfc\", icu.UNormalizationMode2.DECOMPOSE)\n",
    "vi_icu_nfd = normalizer2.normalize(vi_grapheme)\n",
    "print(f'NFD string: {vi_icu_nfd} ({eli.cp(vi_icu_nfd, prefix=True)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Specialised functions:__\n",
    "\n",
    "PyICU provides the following functions to create a Normalizer2 instance:\n",
    "\n",
    "1. `icu.icu.Normalizer2.getNFCInstance()`\n",
    "2. `icu.icu.Normalizer2.getNFKCInstance()`\n",
    "3. `icu.icu.Normalizer2.getNFDInstance()`\n",
    "4. `icu.icu.Normalizer2.getNFKDInstance()`\n",
    "5. `icu.icu.Normalizer2.getNFKCCasefoldInstance()`\n",
    "\n",
    "For NFC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFC string: ậ (U+1EAD)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a PyICU ICU NFC Normalizer2 instance\n",
    "normalizer_nfc = icu.Normalizer2.getNFCInstance()\n",
    "\n",
    "# 2. Normalize string\n",
    "vi_icu_nfc = normalizer_nfc.normalize(vi_grapheme)\n",
    "print(f'NFC string: {vi_icu_nfc} ({eli.cp(vi_icu_nfc, prefix=True)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NFD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFD string: ậ (U+0061 U+0323 U+0302)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a PyICU NFD Normalizer2 instance\n",
    "normalizer_nfd = icu.Normalizer2.getNFDInstance()\n",
    "\n",
    "# 2. Normalize string\n",
    "vi_icu_nfd = normalizer_nfd.normalize(vi_grapheme)\n",
    "print(f'NFD string: {vi_icu_nfd} ({eli.cp(vi_icu_nfd, prefix=True)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that not all graphemes have a precomposed form, therefore such characters are identical in their NFC and NFD forms). If we take the Thuɔŋjäŋ (Dinka) breathy vowel __*ɛ̈*__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canonical equivalents: ['U+025B U+0308']\n",
      "NFC: ɛ̈ (U+025B U+0308)\n",
      "NFD: ɛ̈ (U+025B U+0308)\n"
     ]
    }
   ],
   "source": [
    "din_vowel = \"ɛ̈\"\n",
    "print(f'Canonical equivalents: {eli.canonical_equivalents_str(din_vowel)}')\n",
    "\n",
    "din_nfc = ud.normalize(\"NFC\", din_vowel)\n",
    "print(f\"NFC: {din_nfc} ({eli.cp(din_nfc, prefix=True)})\")\n",
    "din_nfd = ud.normalize(\"NFD\", din_vowel)\n",
    "print(f\"NFD: {din_nfd} ({eli.cp(din_nfd, prefix=True)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence `<U+025B U+0308>` has no canonical equivalents and the NFC and NFD versions of the sequence are identical."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICU transforms\n",
    "\n",
    "The [icu.Transliterator]() class provides flexible and comprehensive text transformations using a single API.\n",
    "\n",
    "It can be used for:\n",
    "\n",
    "* Casing (uppercase, lowercase, and titlecase),\n",
    "* CJK Fullwidth/Halfwidth conversions,\n",
    "* Unicode Normalisation (NFC, NFKC, NFKC_CF, NFD, and NFKD),\n",
    "* Hex and character name conversions, and\n",
    "* Transcription and transliteration conversions.\n",
    "\n",
    "Some of `icu.Transliterator` methods:\n",
    "\n",
    "|Method  |Description  |\n",
    "|------- |------------ |\n",
    "|`icu.Transliterator.createInstance(ID, direction)` |Returns a Transliterator object given its ID. The ID must be either a system transliterator ID or a ID registered using registerInstance(). |\n",
    "|`icu.Transliterator.createFromRules(ID, rules, direction)` |Returns a Transliterator object constructed from the given rule string. This will be a rule-based Transliterator, if the rule string contains only rules, or a compound Transliterator, if it contains ID blocks, or a null Transliterator, if it contains ID blocks which parse as empty for the given direction. |\n",
    "|`icu.Transliterator.createInverse()` |Returns a transliterator's inverse.  |\n",
    "|`icu.Transliterator.getAvailableIDs()` |Return IDs available at the time of the call, including user-registered IDs. |\n",
    "|`icu.Transliterator.registerInstance(instance)` |Registers a Transliterator instance. Must be called before ID used to create an instance.  |\n",
    "\n",
    "### Determining what is supported.\n",
    "\n",
    "ICU uses transliteration transformations defined in [CLDR](https://github.com/unicode-org/cldr/tree/main/common/transforms). The each version of ICU, supports the equivalent version of CLDR, so available transformations will differ form version to version.\n",
    "\n",
    "The function `icu.Transliterator.getAvailableIDs()` will return an `icu.StringEnumeration` object which can be iterated through, providing all the supported transformations. Some transformations will be langauge specific, while others will be more genric and apply to a script.\n",
    "\n",
    "To get a list of transformations involving the Ethiopic scipt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Braille-Ethiopic/Amharic, Cyrillic-Ethiopic/Gutgarts, Cyrl-Ethi/Gutgarts, Ethi-Cyrl/Gutgarts, Ethi-Latn, Ethi-Latn/ALALOC, Ethi-Latn/Aethiopi, Ethi-Latn/Beta_Metsehaf, Ethi-Latn/ES3842, Ethi-Latn/IES_JES_1964, Ethi-Latn/Lambdin, Ethi-Latn/SERA, Ethi-Sarb, Ethi-sgw_Ethi/Gurage_2013, Ethiopic-Braille/Amharic, Ethiopic-Cyrillic/Gutgarts, Ethiopic-Ethiopic/Gurage, Ethiopic-Latin, Ethiopic-Latin/ALALOC, Ethiopic-Latin/Aethiopica, Ethiopic-Latin/Beta_Metsehaf, Ethiopic-Latin/ES3842, Ethiopic-Latin/IES_JES_1964, Ethiopic-Latin/Lambdin, Ethiopic-Latin/SERA, Ethiopic-Latin/Tekie_Alibekit, Ethiopic-Latin/Xaleget, Ethiopic-Musnad, Gurage-Ethiopic, Latin-Ethiopic, Latin-Ethiopic/ALALOC, Latin-Ethiopic/Aethiopica, Latin-Ethiopic/Beta_Metsehaf, Latin-Ethiopic/IES_JES_1964, Latin-Ethiopic/Lambdin, Latin-Ethiopic/SERA, Latin-Ethiopic/Tekie_Alibekit, Latn-Ethi, Latn-Ethi/ALALOC, Latn-Ethi/Aethiopi, Latn-Ethi/Beta_Metsehaf, Latn-Ethi/IES_JES_1964, Latn-Ethi/Lambdin, Latn-Ethi/SERA, Musnad-Ethiopic, Sarb-Ethi, am_Brai-am_Ethi, am_Ethi-am_Brai, am_Ethi-d0_Morse, byn_Ethi-byn_Latn/Tekie_Alibekit, byn_Ethi-byn_Latn/Xaleget, byn_Latn-byn_Ethi/Tekie_Alibekit, d0_Morse-am_Ethi, sgw_Ethi-Ethi/Gurage_2013, Any-Ethi, Any-Ethiopic, Any-Ethiopic/ALALOC, Any-Ethiopic/Beta_Metsehaf, Any-Ethiopic/IES_JES_1964, Any-Ethiopic/Lambdin, Any-Ethiopic/SERA, Any-Ethiopic/Aethiopica, Any-Ethiopic/Tekie_Alibekit, Any-sgw_Ethi/Gurage_2013, Any-am_Ethi, Any-byn_Ethi/Tekie_Alibekit\n"
     ]
    }
   ],
   "source": [
    "# print(\", \".join([*Transliterator.getAvailableIDs()]))\n",
    "\n",
    "def filter_available_transformations(s):\n",
    "    return [x for x in list(icu.Transliterator.getAvailableIDs()) if s.lower() in x.lower()]\n",
    "\n",
    "print(\", \".join(filter_available_transformations(\"ethi\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or search for a variant transformation defined by a specific agency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethi-Latn/ALALOC, Ethiopic-Latin/ALALOC, Latin-Ethiopic/ALALOC, Latn-Ethi/ALALOC, Any-Ethiopic/ALALOC\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(filter_available_transformations(\"ALALOC\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those transformations that are language specific, it is possible to filter for a specific language, for instance to find transforms available for Uzbek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uz_Cyrl-uz/BGN, uz_Cyrl-uz_Latn, uz_Latn-uz_Cyrl, Any-uz_Cyrl, Any-uz_Latn\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(filter_available_transformations(\"uz_\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inbuilt transforms\n",
    "\n",
    "To use ICU's inbuilt transformations:\n",
    "\n",
    "1. Create a transliterator instance using `icu.Transliterator.createInstance()`\n",
    "2. Use the transliterator instance's `transliterate` method on a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नागार्जुन: nāgārjuna\n"
     ]
    }
   ],
   "source": [
    "name_deva = \"नागार्जुन\"\n",
    "\n",
    "# 1. Create a transliterator instance for Devanagari to Latin (ISO 15919)\n",
    "transformer = icu.Transliterator.createInstance(\"Devanagari-Latin\")\n",
    "\n",
    "# 2. Transliterate the text\n",
    "name_latin = transformer.transliterate(name_deva)\n",
    "\n",
    "print(f'{name_deva}: {name_latin}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will convert __नागार्जुन__ to __nāgārjuna__ following the romanisation schema published in ISO 15919. Since Devanagari is unicameral, the romanisation is lowercase. To obtain the transliterated string using sentence casing or title casing, it is necessary to use more complex transformations.\n",
    "\n",
    "Predefined transformations include:\n",
    "\n",
    "1. Script to script transliteration\n",
    "2. Langauge specific transformations\n",
    "3. Casing operations\n",
    "4. Normalisation\n",
    "5. Other text transformations\n",
    "\n",
    "#### Script to script transliteration\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "    <tr>\n",
    "        <th>Script</th>\n",
    "        <th>Transform</th>\n",
    "        <th>Alias</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "<thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Arabic</td>\n",
    "        <td>Arabic-Latin</td>\n",
    "        <td>Arab-Latn</td?>\n",
    "        <td>Default transliteration for Arabic script to Latin script.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Armenian</td>\n",
    "        <td>Armenian-Latin</td>\n",
    "        <td>Armn-Latn</td>\n",
    "        <td>Default transliteration for Armenian script to Latin script.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"10\">Bengali</td>\n",
    "        <td></td>\n",
    "        <td>Beng-Arab</td>\n",
    "        <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Beng-Deva</td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Beng-Gujr</td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Beng-Guru</td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td>Beng-Kndau</td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td></td>\n",
    "        <td></td>\n",
    "    <tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Bengali-Arabic\n",
    "Bengali-Devanagari\n",
    "Bengali-Gujarati\n",
    "Bengali-Gurmukhi\n",
    "Bengali-Kannada\n",
    "Bengali-Latin\n",
    "Bengali-Malayalam\n",
    "Bengali-Oriya\n",
    "Bengali-Tamil\n",
    "Bengali-Telugu\n",
    "\n",
    "\n",
    "\n",
    "#### Language specific transliterations\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "    <tr>\n",
    "        <th>Language</th>\n",
    "        <th>Transform</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "<thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td>Amharic</td>\n",
    "        <td>Amharic-Latin/BGN</td>\n",
    "        <td>BGN/PCGN romanization for <a href=\"https://geonames.nga.mil/geonames/GNSSearch/GNSDocs/romanization/ROMANIZATION_OF_AMHARIC.pdf\">Amharic language</a></td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>Arabic</td>\n",
    "        <td>Arabic-Latin/BGN</td>\n",
    "        <td>BGN/PCGN romanization for <a href=\"https://geonames.nga.mil/geonames/GNSSearch/GNSDocs/romanization/Romanization_of_Arabic_2019.pdf\">Arabic language</a></td>\n",
    "    <tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "BGN/PCGN romanization are the conventions used by the United States Board on Geographic Names (BGN) and the Permanent Committee on Geographical Names for British Official Use (PCGN).\n",
    "\n",
    "#### Casing, Normalisation, and other transformations\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "    <tr>\n",
    "        <th>Category</th>\n",
    "        <th>Transform</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "<thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "        <td rowspan=\"16\">Casing</td>\n",
    "        <td>Any-Lower</td>\n",
    "        <td rowspan=\"3\">Simple casing</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-Upper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-Title</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>az-Lower</td>\n",
    "        <td rowspan=\"3\">Full casing (Azeri)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>az-Upper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>az-Title</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>el-Lower</td>\n",
    "        <td rowspan=\"3\">Full casing (Greek)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>el-Upper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>el-Title</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>lt-Lower</td>\n",
    "        <td rowspan=\"3\">Full casing (Lithuanian)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>lt-Upper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>lt-Title</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>nl-Title</td>\n",
    "        <td>Full Title casing (Dutch)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tr-Lower</td>\n",
    "        <td rowspan=\"3\">Full casing (Turkish)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tr-Upper</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>tr-Title</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"2\">CJK transformations</td>\n",
    "        <td>Fullwidth-Halfwidth</td>\n",
    "        <td rowspan=\"2\">Convert between fullwidth and halfwidth charcaters </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Halfwidth-Fullwidth</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"6\">Normalisation</td>\n",
    "        <td>Any-NFC</td>\n",
    "        <td rowspan=\"6\">Unicode normalisation</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-NFKC</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-NFD</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-NFKD</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-FCD</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Any-FCC</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Any-Hex\n",
    "Any-Hex/Unicode\n",
    "Any-Hex/Java\n",
    "Any-Hex/C\n",
    "Any-Hex/XML\n",
    "Any-Hex/XML10\n",
    "Any-Hex/Perl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "नागार्जुन\n",
      "U+0928 U+093E U+0917 U+093E U+0930 U+094D U+091C U+0941 U+0928\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a PyICU Transliterator Instance\n",
    "transformer_u = icu.Transliterator.createInstance(\"Any-Hex/Unicode\")\n",
    "\n",
    "# 2. Transliterate the text\n",
    "name_cp = transformer_u.transliterate(name_deva)\n",
    "unicode_list = \" \".join([\"U\"+x for x in name_cp.split(\"U\") if x])\n",
    "print(f'{name_deva}\\n{unicode_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom rules\n",
    "\n",
    "```py\n",
    "icu.Transliterator.createFromRules(label, rules, direction)\n",
    "```\n",
    "\n",
    "Where: \n",
    "\n",
    "__label__: identifier for the transform. \\\n",
    "__rules__: string containing rules used to build Transliterator instance \\\n",
    "__direction__: direction of transformation, either icu.UTransDirection.FORWARD or icu.UTransDirection.REVERSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dɛteicekaŋ Akɔɔn\n"
     ]
    }
   ],
   "source": [
    "wp_title = \"Dɛ̈tëicëkäŋ akɔ̈ɔ̈n\"\n",
    "transformer_rules = ':: NFD; :: [\\u0308] Remove; :: Title; '\n",
    "custom_transformer = icu.Transliterator.createFromRules(\"customDinka\", transformer_rules, icu.UTransDirection.FORWARD)\n",
    "print(custom_transformer.transliterate(wp_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transform will daisy chain two inbuilt tarnsformations and a custom transformation:\n",
    "\n",
    "1. Normalised string to NFD\n",
    "2. Remove combining any combining diareses (U+0308), using ICU's UnicodeSet notation\n",
    "3. Title case string\n",
    "\n",
    "Much more complex transformations are possible, and it is possible to create rules that will run a range of text transformations on strings, allowing a range of data cleanup functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a transformation\n",
    "\n",
    "When using a custom Transliterator instance within a web microframework, an API endpoint or other scenarios where code persists, rather than recreating the instance each time, it can be created, registered and then used the same way ICU internal transformations are used.\n",
    "\n",
    "1. Create a custom Transliterator instance\n",
    "2. Register instance\n",
    "\n",
    "Use the following command:\n",
    "\n",
    "```py\n",
    "icu.Transliterator.registerInstance(instance)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [icu::Transliterator Class Reference](https://unicode-org.github.io/icu-docs/apidoc/dev/icu4c/classicu_1_1Transliterator.html) (icu4c)\n",
    "* [ICU User guide: Transforms](https://unicode-org.github.io/icu/userguide/transforms/)\n",
    "* [Transform Rule Tutorial](https://unicode-org.github.io/icu/userguide/transforms/general/rules.html)\n",
    "* [Unicode Locale Data Markup Language (UTS 35): Transforms](https://unicode.org/reports/tr35/tr35-general.html#Transforms)\n",
    "* [Transformations defined in CLDR](https://github.com/unicode-org/cldr/tree/main/common/transforms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb12d0de9674b66c629d2bafada2ec4f6e6dba6d129e54dea4badc21502d54d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
